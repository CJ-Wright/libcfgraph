{
 "about": {
  "channels": [
   "conda-forge",
   "defaults"
  ],
  "conda_build_version": "3.16.3",
  "conda_private": false,
  "conda_version": "4.5.11",
  "description": "Use airflow to author workflows as directed acyclic graphs (DAGs)\nof tasks. The airflow scheduler executes your tasks on an array of\nworkers while following the specified dependencies. Rich command\nline utilities make performing complex surgeries on DAGs a snap.\nThe rich user interface makes it easy to visualize pipelines\nrunning in production, monitor progress, and troubleshoot issues\nwhen needed.\n\nWhen workflows are defined as code, they become more maintainable,\nversionable, testable, and collaborative.\n",
  "dev_url": "https://github.com/apache/incubating-airflow",
  "doc_url": "http://pythonhosted.org/airflow/profiling.html",
  "env_vars": {
   "CIO_TEST": "<not set>"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "parent_recipe": {
    "name": "airflow-split",
    "path": "/home/conda/recipe_root",
    "version": "1.10.1"
   },
   "recipe-maintainers": [
    "sodre",
    "halldc"
   ]
  },
  "home": "http://airflow.apache.org",
  "identifiers": [],
  "keywords": [],
  "license": "Apache 2.0",
  "license_file": "LICENSE",
  "root_pkgs": [
   "filelock 3.0.10 py_0",
   "six 1.11.0 py36_1001",
   "bzip2 1.0.6 h470a237_2",
   "beautifulsoup4 4.6.3 py36_1000",
   "asn1crypto 0.24.0 py36_1003",
   "python-libarchive-c 2.8 py36_1004",
   "libedit 3.1.20170329 haf1bffa_1",
   "readline 7.0 haf1bffa_1",
   "anaconda-client 1.7.1 py_0",
   "xz 5.2.4 h470a237_1",
   "ncurses 6.1 hfc679d8_1",
   "pycosat 0.6.3 py36h470a237_1",
   "idna 2.7 py36_1002",
   "libiconv 1.15 h470a237_3",
   "yaml 0.1.7 h470a237_1",
   "markupsafe 1.1.0 py36h470a237_0",
   "pycparser 2.19 py_0",
   "attrs 18.2.0 py_0",
   "zlib 1.2.11 h470a237_3",
   "tini 0.18.0 h470a237_1",
   "pyyaml 3.13 py36h470a237_1",
   "pytz 2018.7 py_0",
   "sqlite 3.25.3 hb1c47c0_0",
   "cryptography-vectors 2.3.1 py36_1000",
   "pysocks 1.6.8 py36_1002",
   "decorator 4.3.0 py_0",
   "glob2 0.6 py_0",
   "psutil 5.4.8 py36h470a237_0",
   "urllib3 1.23 py36_1001",
   "python-dateutil 2.7.5 py_0",
   "jinja2 2.10 py_1",
   "nbformat 4.4.0 py_1",
   "chardet 3.0.4 py36_1003",
   "traitlets 4.3.2 py36_1000",
   "conda-env 2.6.0 1",
   "wheel 0.32.3 py36_0",
   "pkginfo 1.4.2 py_1",
   "patchelf 0.9 hfc679d8_2",
   "tk 8.6.9 ha92aebf_0",
   "pip 18.1 py36_1000",
   "perl 5.26.2 h470a237_0",
   "pyrsistent 0.14.6 py36h470a237_0",
   "ipython_genutils 0.2.0 py_1",
   "ruamel_yaml 0.15.71 py36h470a237_0",
   "libffi 3.2.1 hfc679d8_5",
   "jupyter_core 4.4.0 py_0",
   "tqdm 4.28.1 py_0",
   "cffi 1.11.5 py36h5e8e0c9_1",
   "pyopenssl 18.0.0 py36_1000",
   "expat 2.2.5 hfc679d8_2",
   "jsonschema 3.0.0a3 py36_1000",
   "requests 2.20.1 py36_1000",
   "gettext 0.19.8.1 h5e8e0c9_1",
   "gosu 1.10 h81701ea_1001",
   "setuptools 40.6.2 py36_0",
   "clyent 1.2.2 py_1",
   "conda 4.5.11 py36_0",
   "zstd 1.3.7 h0b5b093_0",
   "cryptography 2.4.1 py36h1ba5d50_0",
   "libssh2 1.8.0 h1ba5d50_4",
   "conda-build 3.16.3 py36_0",
   "libcurl 7.62.0 h20c2e04_0",
   "libgcc-ng 8.2.0 hdf63c60_1",
   "conda-forge-ci-setup 2.0.8 py36_0",
   "libxml2 2.9.8 h26e45fe_1",
   "curl 7.62.0 hbc83047_0",
   "git 2.8.2 0",
   "lz4-c 1.8.1.2 h14c3975_0",
   "libarchive 3.3.3 h5d8350f_4",
   "certifi 2018.10.15 py36_0",
   "icu 58.2 h9c2bf20_1",
   "python 3.6.7 h0371630_0",
   "openssl 1.1.1a h7b6447c_0",
   "ca-certificates 2018.03.07 0",
   "lzo 2.10 h49e0be7_2",
   "krb5 1.16.1 h173b8e3_7",
   "libstdcxx-ng 8.2.0 hdf63c60_1",
   "click 7.0 py36_0"
  ],
  "summary": "Airflow is a platform to programmatically author, schedule and monitor workflows",
  "tags": []
 },
 "conda_build_config": {
  "build_number_decrement": "0",
  "c_compiler": "gcc",
  "channel_sources": "conda-forge,defaults",
  "channel_targets": "conda-forge main",
  "cpu_optimization_target": "nocona",
  "cran_mirror": "https://cran.r-project.org",
  "cxx_compiler": "gxx",
  "docker_image": "condaforge/linux-anvil",
  "fortran_compiler": "gfortran",
  "ignore_build_only_deps": "numpy",
  "lua": "5",
  "numpy": "1.11",
  "perl": "5.26.0",
  "pin_run_as_build": {
   "python": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   },
   "r-base": {
    "max_pin": "x.x.x",
    "min_pin": "x.x.x"
   }
  },
  "python": "2.7",
  "r_base": "3.5",
  "target_platform": "linux-64"
 },
 "files": [],
 "index": {
  "arch": "x86_64",
  "build": "0",
  "build_number": 0,
  "depends": [
   "airflow >=1.10.1,<1.10.2.0a0",
   "python-hdfs >=0.5.1"
  ],
  "license": "Apache 2.0",
  "name": "airflow-with-webhdfs",
  "platform": "linux",
  "subdir": "linux-64",
  "timestamp": 1542874203260,
  "version": "1.10.1"
 },
 "metadata_version": 1,
 "name": "airflow-with-webhdfs",
 "raw_recipe": "# This file created by conda-build 3.16.3\n# ------------------------------------------------\n\npackage:\n    name: airflow-with-webhdfs\n    version: 1.10.1\nsource:\n    fn: airflow-1.10.1.tar.gz\n    sha256: 9fe8def33ba50e9f6c3f53ef25dbf986ebf6ed857409a338208c29b08de200e4\n    url: https://github.com/apache/incubator-airflow/archive/1.10.1.tar.gz\nbuild:\n    noarch: false\n    number: '0'\n    string: '0'\nrequirements:\n    run:\n        - airflow >=1.10.1,<1.10.2.0a0\n        - python-hdfs >=0.5.1\nabout:\n    description: 'Use airflow to author workflows as directed acyclic graphs (DAGs)\n\n        of tasks. The airflow scheduler executes your tasks on an array of\n\n        workers while following the specified dependencies. Rich command\n\n        line utilities make performing complex surgeries on DAGs a snap.\n\n        The rich user interface makes it easy to visualize pipelines\n\n        running in production, monitor progress, and troubleshoot issues\n\n        when needed.\n\n\n        When workflows are defined as code, they become more maintainable,\n\n        versionable, testable, and collaborative.\n\n        '\n    dev_url: https://github.com/apache/incubating-airflow\n    doc_url: http://pythonhosted.org/airflow/profiling.html\n    home: http://airflow.apache.org\n    license: Apache 2.0\n    license_file: LICENSE\n    summary: Airflow is a platform to programmatically author, schedule and monitor\n        workflows\nextra:\n    copy_test_source_files: true\n    final: true\n    recipe-maintainers:\n        - halldc\n        - sodre\n",
 "rendered_recipe": {
  "about": {
   "description": "Use airflow to author workflows as directed acyclic graphs (DAGs)\nof tasks. The airflow scheduler executes your tasks on an array of\nworkers while following the specified dependencies. Rich command\nline utilities make performing complex surgeries on DAGs a snap.\nThe rich user interface makes it easy to visualize pipelines\nrunning in production, monitor progress, and troubleshoot issues\nwhen needed.\n\nWhen workflows are defined as code, they become more maintainable,\nversionable, testable, and collaborative.\n",
   "dev_url": "https://github.com/apache/incubating-airflow",
   "doc_url": "http://pythonhosted.org/airflow/profiling.html",
   "home": "http://airflow.apache.org",
   "license": "Apache 2.0",
   "license_file": "LICENSE",
   "summary": "Airflow is a platform to programmatically author, schedule and monitor workflows"
  },
  "build": {
   "noarch": false,
   "number": "0",
   "string": "0"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "halldc",
    "sodre"
   ]
  },
  "package": {
   "name": "airflow-with-webhdfs",
   "version": "1.10.1"
  },
  "requirements": {
   "run": [
    "airflow >=1.10.1,<1.10.2.0a0",
    "python-hdfs >=0.5.1"
   ]
  },
  "source": {
   "fn": "airflow-1.10.1.tar.gz",
   "sha256": "9fe8def33ba50e9f6c3f53ef25dbf986ebf6ed857409a338208c29b08de200e4",
   "url": "https://github.com/apache/incubator-airflow/archive/1.10.1.tar.gz"
  }
 },
 "version": "1.10.1"
}